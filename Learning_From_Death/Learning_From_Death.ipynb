{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning From Death Script\n",
    "\n",
    "- Version 1.4 - Clinical Narrative Added For NHS Pycom - 22/05/2022\n",
    "- Version 1.3.1 - Bugs fixed and versioned such that source code can be safely made open source - 23/04/2022\n",
    "- Version 1.2.3 - Changes made to improve cohort capture mechanism - 23/03/2022\n",
    "- Version 1.2.2 - Process Automated - 02/03/2022\n",
    "- Version 1.2.1 - Improved upon initial version with better filtering, output formatting and direct database pulls - 13/02/2022\n",
    "- Version 1.1.2 - Soft Launch. Used alongside conventional M&M process to compare - 14/01/2022\n",
    "- Version 1.1.1 - Most Data Pulls in Place - 10/12/2021\n",
    "\n",
    "#### Authors:\n",
    "\n",
    "1. Matt Stammers - Consultant Gastroenterolgist and Data Scientist, UHS\n",
    "2. Michael George - Data Engineering Lead, UHS\n",
    "\n",
    "What this Script Does:\n",
    "- Finds patients who have died during a particular admission.\n",
    "- Obtains key risk factors for death and key information documenting the admission.\n",
    "- Risk stratifies patient mortality by validated risk scoring systems (CCI and HFRS) using comorbidipy (see documentation).\n",
    "- Deposit the results into a flat file on the network for use by clinical teams for the purposes of morbidity and mortality estimation to help improve the quality of discussion and visibility for those involved in the direct clinical care of the patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Packages\n",
    "\n",
    "Below are the key packages needed to run this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime Packages\n",
    "import datetime as datetime\n",
    "\n",
    "# Database Connectors\n",
    "import cx_Oracle as cxo\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy import create_engine, MetaData, Table, and_\n",
    "from sqlalchemy.sql import select\n",
    "\n",
    "# Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Encryption\n",
    "import keyring\n",
    "\n",
    "# Risk Scoring\n",
    "from comorbidipy import comorbidity\n",
    "from comorbidipy import hfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on Flat Files\n",
    "\n",
    "If you want to run flat files instead of connecting to the database you should do so here. For instance if you have a .csv file or set of .csv files with the relevant data in them you can import them as per the following:\n",
    "\n",
    "```python\n",
    "df_comorb = pd.read_csv('{path to file}/comorbidity_data.csv')\n",
    "```\n",
    "\n",
    "We would however recommend setting up a database connection to this as it is going to be far more scalable as you will see below. You will need your local IT team's help to set this up but once you have access credentials they can be stored using keyring as below:\n",
    "\n",
    "```python\n",
    "keyring.set_password('User', '1', 'jimminycrickets')\n",
    "```\n",
    "\n",
    "Once you have set this up as long as you remember how you stored everything it can easily be retrieved by switching set to get_password as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on connecting to a database\n",
    "\n",
    "In this example we are connecting to an Oracle SQL database using the SQLAlchemy library.\n",
    "\n",
    "One method of connecting your database via SQLAlchemy is using an Engine. This is done by passing in a 'Database URL' to the create_engine() function. These database URLS follow a particular protocol and generally include the username, password, hostname and database name, and some other optional keyword arguments for additional configuration (in our example we have included a service_name configuration option to connect to the Oracle database)\n",
    "\n",
    "A typical database url looks like this:\n",
    "\n",
    "```python\n",
    "dialect+driver://username:password@host:port/database\n",
    "```\n",
    "\n",
    "For more details you can refer to the SQLAlchemy documentation regarding engines and how to construct a database url for your particualr hospitals database: https://docs.sqlalchemy.org/en/14/core/engines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print version of sqlalchemy\n",
    "print(sqla.__version__)  \n",
    "\n",
    "# Print if the cx_Oracle is recognized\n",
    "print(cxo.version)   \n",
    "\n",
    "# Setup Connection to Client\n",
    "\n",
    "cxo.init_oracle_client(lib_dir= \"{path to client}/instantclient_11_2/\")\n",
    "\n",
    "# Print client version\n",
    "print(cxo.clientversion())\n",
    "\n",
    "# Load in Connection Credentials\n",
    "ora_user = keyring.get_password(\"User\", \"10\")\n",
    "ora_password = keyring.get_password(\"Password\", \"10\")\n",
    "ora_host = keyring.get_password(\"Host\", \"10\")\n",
    "ora_service = keyring.get_password(\"Service\", \"10\")\n",
    "ora_port = keyring.get_password(\"Port\", \"10\")\n",
    "\n",
    "# Set Key Connection Variables\n",
    "\n",
    "DIALECT = 'oracle'\n",
    "SQL_DRIVER = 'cx_oracle'\n",
    "USERNAME = ora_user\n",
    "PASSWORD = ora_password\n",
    "HOST = ora_host\n",
    "PORT = ora_port\n",
    "SERVICE = ora_service\n",
    "\n",
    "# Create Engine Authorisation String Without Exposing Credentials\n",
    "ENGINE_PATH_WIN_AUTH = f'{DIALECT}+{SQL_DRIVER}://{USERNAME}:{PASSWORD}@{HOST}:{str(PORT)}/?service_name={SERVICE}'\n",
    "# ENGINE_PATH_WIN_AUTH = DIALECT + '+' + SQL_DRIVER + '://' + USERNAME + ':' + PASSWORD +'@' + HOST + ':' + str(PORT) + '/?service_name=' + SERVICE\n",
    "\n",
    "# Create and Connect to Engine\n",
    "\n",
    "engine = create_engine(ENGINE_PATH_WIN_AUTH)\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note when learning\n",
    "\n",
    "When you are learning to do the above we recommend breaking this block up into smaller subcomponents until you have mastered each of them. It will be worth the effort to learn how to do this. Once you have done it you can then connect to your SQL Queries\n",
    "\n",
    "#### SQL Queries and Extract\n",
    "\n",
    "These SQL queries are pseudocode to help you get the idea behind the approach and why it is being achieved this way. If you however insert the SQL in this format between the comments and then parse it through the engine it will work. For the sake of simplicity I have generated individual queries for the different components however some of the connections could be made database-side but we are deliberately assuming little SQL knowledge in this script to make the process easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds all the patients who have died under x team within x timeframe:\n",
    "\n",
    "Mortality_Query = \"\"\"\n",
    "SELECT {insert columns of interest}\n",
    "FROM {main table}\n",
    "{LEFT/INNER/OUTER} JOIN {insert tables of interest}\n",
    "WHERE {filters of interest - in this case patients admitted under my team who died within x months of now}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you can create your index cohort\n",
    "\n",
    "From this you can filter the subsequent queries. This is often better done using SQL itself but if you do want to do it using python this will work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets you the mortality cohort\n",
    "\n",
    "df_died = pd.read_sql_query(Mortality_Query, engine)\n",
    "\n",
    "# You can then clean up the patient identifiers if needed and turn them into a tuple as below.\n",
    "# You might need to set them first if there are repeats but if not this will work:\n",
    "\n",
    "patients_who_died = tuple(df_died['patient_id'].to_list())\n",
    "\n",
    "# Now we have our index list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now run the other queries in sequence \n",
    "\n",
    "This enables you to gather your cohort with the first query and then cross-filter the results with subsequent queries. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds key patient demographics\n",
    "\n",
    "Demographics_Query = \"\"\"\n",
    "SELECT {insert columns of interest - probably patient table or like}\n",
    "FROM {main presumably patient table}\n",
    "{LEFT/INNER/OUTER} JOIN {insert tables of interest}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Mortality Query}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "\"\"\".format(patients_who_died)\n",
    "\n",
    "# Comorbidity Query\n",
    "\n",
    "Comorbidity_Query = \"\"\"\n",
    "SELECT {insert columns of interest - likely ICD10/SNOMED codes}\n",
    "FROM {main ICD10/SNOMED table}\n",
    "{LEFT/INNER/OUTER} JOIN {other important ICD10/SNOMED tables}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Mortality Query}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "ORDER BY {perhaps by date or code}\n",
    "\"\"\".format(patients_who_died)\n",
    "\n",
    "# Narrative Query\n",
    "\n",
    "Narrative_Query = \"\"\"\n",
    "SELECT {insert columns of interest - likely discharge summary free text}\n",
    "FROM {main discharge summary table}\n",
    "{LEFT/INNER/OUTER} JOIN {other important tables if needed}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Mortality Query}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "\"\"\".format(patients_who_died)\n",
    "\n",
    "# Physiology Query\n",
    "\n",
    "Physiology_Query = \"\"\"\n",
    "SELECT {insert columns of interest - likely weight and height rather than BMI}\n",
    "FROM {main physiology data table}\n",
    "{LEFT/INNER/OUTER} JOIN {probably not required}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Mortality Query and perhaps time}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "\"\"\".format(patients_who_died)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can now connect your engine to these queries to create the dataframes\n",
    "\n",
    "Now by connecting the engines to the queries you can pull all relevant data through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes it very easy to pull discrete up to date datasets from your EPR/PAS into the kernel\n",
    "\n",
    "df_demographics = pd.read_sql_query(Demographics_Query, engine)\n",
    "df_comorbidity = pd.read_sql_query(Comorbidity_Query, engine)\n",
    "df_narrative = pd.read_sql_query(Narrative_Query, engine)\n",
    "df_physiology = pd.read_sql_query(Physiology_Query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next I calculated BMI\n",
    "\n",
    "This might be done for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First extract the weights and heights from the data to create two seperate pandas series\n",
    "\n",
    "df_heights = df_physiology[df_physiology['test_code'] == 'HEIG']\n",
    "df_weights = df_physiology[df_physiology['test_code'] == 'WEIG']\n",
    "\n",
    "# Then aggregate them - we settled for mean in the end after checking for skewness\n",
    "\n",
    "df_heights2 = df_heights.groupby(['patient_id']).agg('mean').reset_index()\n",
    "df_weights2 = df_weights.groupby(['patient_id']).agg('mean').reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "\n",
    "df_weights2.columns = ['PATIENT_ID', 'MEAN_WEIGHT']\n",
    "df_heights2.columns = ['PATIENT_ID', 'MEAN_HEIGHT']\n",
    "\n",
    "# Join Together\n",
    "df_bmi = pd.concat([df_heights2, df_weights2])\n",
    "\n",
    "# Then convert height to meters if currently in centimeters and calc BMI\n",
    "\n",
    "df_bmi['MEAN_HEIGHT_M'] = df_bmi['MEAN_HEIGHT']/100\n",
    "df_bmi['BMI'] = round(df_bmi['MEAN_WEIGHT']/(df_bmi['MEAN_HEIGHT_M'])**2,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now join physiology to the core cohort and demographics table to make a first merged dataframe\n",
    "\n",
    "A process of joining datasets when in a one to one relationship keeps things tidy. In this script we assume that the demographics table contains one id per patient - if this is not the case you will need to clean that first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First merge the core cohort to the demographics and then add the BMI data\n",
    "\n",
    "df_merge1 = pd.merge(df_died, df_demographics, on='patient_id', how = 'left')\n",
    "df_merge2 = pd.merge(df_merge1, df_bmi, on='patient_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now calculate the comorbidity scores\n",
    "\n",
    "We can now apply our risk algorithm as we have age, comorbidities and unique identifiers in one table also with the BMI if needed (it acts as an independent marker of risk but doesn't feature in CCI or HFRS per se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a subframe with the key columns\n",
    "\n",
    "comorbidities = df_merge2[['code', 'patient_id', 'age']]\n",
    "comorbidities.columns = ['code', 'id', 'age']\n",
    "\n",
    "# Then calculate the scores. Comorbidipy needs them in this format to work properly\n",
    "\n",
    "cci = comorbidity(comorbidities)\n",
    "frail = hfrs(comorbidities)\n",
    "\n",
    "# Then tidy up the outputs a bit\n",
    "\n",
    "cci['survival_10yr'] = round(cci['survival_10yr']*100,1).astype(str)\n",
    "cci['survival_10yr'] = cci['survival_10yr'].apply(lambda x: ''.join(x + \"%\"))\n",
    "\n",
    "# Then join these tables into one and add back to the main dataframe to create merge3\n",
    "\n",
    "comorb_merge = pd.merge(cci, frail, on='id', how='left')\n",
    "df_merge3_added = pd.merge(df_merge2, comorb_merge, left_on='patient_id', right_on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now add the narrative data\n",
    "\n",
    "The narrative data has been left till last because it is the most difficult to handle. It is many to one and thus needs to be filtered on the dates to make sure the correct narrative aligns with the correct admission. You can also add the cause of death but remember this information is highly sensitive so has to be stored very carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DateTime\n",
    "\n",
    "To make the process of converting the datetimes easier you can use a simple function like this which converts particular columns to datetime values based on only the name of the column. Be careful though that the dates are all formatted the same or you might get odd results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime Function - to turn all column strings containing 'date' or 'datetime' into datetimes\n",
    "\n",
    "def Datetime(series):\n",
    "    if ('date' in series.name.lower() or 'datetime' in series.name.lower()) and 'age' not in series.name.lower():\n",
    "            series = pd.to_datetime(series, dayfirst = True)\n",
    "    return series\n",
    "\n",
    "df_narrative = df_narrative.apply(Datetime)\n",
    "df_died = df_died.apply(Datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then Filter the narratives to make sure they match the correct admission narrative\n",
    "\n",
    "Simply by joining the mortality to the narratives and then using the .between() method we can filter the narratives to make sure they match the correct admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join them all together\n",
    "\n",
    "df_narratives = pd.merge(df_died, df_narrative, on='patient_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filter to select only the correct narratives out\n",
    "\n",
    "df_filtered_narratives = df_narratives[df_narratives['date_of_death'].between(df_narratives['admission_date'], df_narratives['discharge_date'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining all together\n",
    "\n",
    "This creates a final dataframe. By this point the data is probably a bit unwieldy and needs truncating. How you do this is up to you but a simple method for truncating / rearranging columns is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final Merge\n",
    "\n",
    "df_merge4 = pd.merge(df_merge3, df_filtered_narratives, on='patient_id', how='left')\n",
    "\n",
    "# Final Output\n",
    "\n",
    "final = df_merge4[['patient_id', 'age', 'survival_10yr', 'hfrs', 'admission narrative', 'cause of death']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Output\n",
    "\n",
    "Typically in hospitals this process will be managed by team consultant and perhaps middle grade. We set up a system where by the data could be output to a particular secure location and accessed by only those directly involved in preparing for the M&M meeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate timedeltas for the output files\n",
    "\n",
    "year_ago_today = pd.to_datetime('today') - timedelta(days=365.25)\n",
    "year_ago_today_string = year_ago_today.date()\n",
    "date_today_string = pd.to_datetime('today').date()\n",
    "\n",
    "# Output the file\n",
    "\n",
    "final.to_csv('{path to output location}' + 'Team x Deaths Between {} and {}.csv'.format(year_ago_today_string, date_today_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you think?\n",
    "\n",
    "We share this as an example of how one can easily use python to dramatically simplify a fairly arduous & complex task into something far more manageable and useful. Internally this process cut the time for mortality meeting preparation by about 50% because it removed a lot of the data gathering steps that were previously required. As you can see it is not a particularly complicated task using simple python scripting.\n",
    "\n",
    "Feel free to contact me at matt@reallyusefulmodels.com if you have any further queries or questions about this, need help or simply want to point out potential improvements / share how you have built upon this."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3aee4e6f83186305632a7da38ab510cf63325fa2053007e001204ef8de185803"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
