{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protecting Life Data Preparation Script\n",
    "\n",
    "- Version 1.2.1 - NHS Pycom Version Built - 22/05/2022\n",
    "- Version 1.1.3 - Current Version Demo delivered to Divisional Management - 14/04/2022\n",
    "- Version 1.1.2 - Abstract Submitted to BSG Conference 2022 - 25/02/2022\n",
    "- Version 1.1.1 - Basic MVP Built - 23/02/2022\n",
    "\n",
    "#### Authors:\n",
    "\n",
    "1. Matt Stammers - Consultant Gastroenterolgist and Data Scientist @ AXIS, UHS\n",
    "2. Michael George - Data Engineering Lead @ AXIS, UHS\n",
    "\n",
    "What this Script Does:\n",
    "- Finds patients who have suffered an acute gastrointestinal bleed and identifies their index admission.\n",
    "- Obtains key risk factors for death and key information documenting the admission.\n",
    "- Risk stratifies patient mortality by validated risk scoring systems (CCI and HFRS) using comorbidipy (see documentation).\n",
    "- Calculates all the other necessary covariates and outcome variables required for analysis.\n",
    "- Deposit the results into a flat file on the network for use by clinical teams or for further subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime Packages\n",
    "import datetime as datetime\n",
    "\n",
    "# Database Connectors\n",
    "import cx_Oracle as cxo\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy import create_engine, MetaData, Table, and_\n",
    "from sqlalchemy.sql import select\n",
    "\n",
    "# Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Encryption\n",
    "import keyring\n",
    "\n",
    "# Risk Scoring\n",
    "from comorbidipy import comorbidity\n",
    "from comorbidipy import hfrs\n",
    "\n",
    "# Print version of sqlalchemy\n",
    "print(sqla.__version__)  \n",
    "\n",
    "# Print if the cx_Oracle is recognized\n",
    "print(cxo.version)   \n",
    "\n",
    "# Setup Connection to Client\n",
    "\n",
    "cxo.init_oracle_client(lib_dir= \"{path to client}/instantclient_11_2/\")\n",
    "\n",
    "# Print client version\n",
    "print(cxo.clientversion())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on Flat Files\n",
    "\n",
    "If you want to run flat files instead of connecting to the database you should do so here. For instance if you have a .csv file or set of .csv files with the relevant data in them you can import them as per the following:\n",
    "\n",
    "```python\n",
    "df_comorb = pd.read_csv('{path to file}/comorbidity_data.csv')\n",
    "```\n",
    "\n",
    "We would however recommend setting up a database connection to this as it is going to be far more scalable as you will see below. You will need your local IT team's help to set this up but once you have access credentials they can be stored using keyring as below:\n",
    "\n",
    "```python\n",
    "keyring.set_password('User', '1', 'jimminycrickets')\n",
    "```\n",
    "\n",
    "Once you have set this up as long as you remember how you stored everything it can easily be retrieved by switching set to get_password as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on connecting to a database\n",
    "\n",
    "In this example we are connecting to an Oracle SQL database using the SQLAlchemy library.\n",
    "\n",
    "One method of connecting your database via SQLAlchemy is using an Engine. This is done by passing in a 'Database URL' to the create_engine() function. These database URLS follow a particular protocol and generally include the username, password, hostname and database name, and some other optional keyword arguments for additional configuration (in our example we have included a service_name configuration option to connect to the Oracle database)\n",
    "\n",
    "A typical database url looks like this:\n",
    "\n",
    "```python\n",
    "dialect+driver://username:password@host:port/database\n",
    "```\n",
    "\n",
    "For more details you can refer to the SQLAlchemy documentation regarding engines and how to construct a database url for your particualr hospitals database: https://docs.sqlalchemy.org/en/14/core/engines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Connection Credentials\n",
    "\n",
    "ora_user = keyring.get_password(\"User\", \"10\")\n",
    "ora_password = keyring.get_password(\"Password\", \"10\")\n",
    "ora_host = keyring.get_password(\"Host\", \"10\")\n",
    "ora_service = keyring.get_password(\"Service\", \"10\")\n",
    "ora_port = keyring.get_password(\"Port\", \"10\")\n",
    "\n",
    "# Set Key Connection Variables\n",
    "\n",
    "DIALECT = 'oracle'\n",
    "SQL_DRIVER = 'cx_oracle'\n",
    "USERNAME = ora_user\n",
    "PASSWORD = ora_password\n",
    "HOST = ora_host\n",
    "PORT = ora_port\n",
    "SERVICE = ora_service\n",
    "\n",
    "# Create Engine Authorisation String Without Exposing Credentials\n",
    "ENGINE_PATH_WIN_AUTH = f'{DIALECT}+{SQL_DRIVER}://{USERNAME}:{PASSWORD}@{HOST}:{str(PORT)}/?service_name={SERVICE}'\n",
    "# ENGINE_PATH_WIN_AUTH = DIALECT + '+' + SQL_DRIVER + '://' + USERNAME + ':' + PASSWORD +'@' + HOST + ':' + str(PORT) + '/?service_name=' + SERVICE\n",
    "\n",
    "# Create and Connect to Engine\n",
    "\n",
    "engine = create_engine(ENGINE_PATH_WIN_AUTH)\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note when learning\n",
    "\n",
    "When you are learning to do the above we recommend breaking this block up into smaller subcomponents until you have mastered each of them. It will be worth the effort to learn how to do this. Once you have done it you can then connect to your SQL Queries\n",
    "\n",
    "#### SQL Queries and Extract\n",
    "\n",
    "These SQL queries are pseudocode to help you get the idea behind the approach and why it is being achieved this way. If you however insert the SQL in this format between the comments and then parse it through the engine it will work. For the sake of simplicity I have generated individual queries for the different components however some of the connections could be made database-side but we are deliberately assuming little SQL knowledge in this script to make the process easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds all the patients who have had a gastrointestinal bleed within the hospital\n",
    "\n",
    "GI_Bleed_Query = \"\"\"\n",
    "SELECT {insert columns of interest}\n",
    "FROM {endoscopy table}\n",
    "{LEFT/INNER/OUTER} JOIN {insert tables of interest}\n",
    "WHERE {filters of interest - in this case patients with meleana, haematemesis or 'coffee ground vomiting'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you can create your index cohort\n",
    "\n",
    "From this you can filter the subsequent queries. This is often better done using SQL itself but if you do want to do it using python this will work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets you the GI Bleed cohort\n",
    "\n",
    "df_bleeding = pd.read_sql_query(GI_Bleed_Query, engine)\n",
    "\n",
    "# Patients who bled\n",
    "\n",
    "patients_who_bled = tuple(df_bleeding['patient_id'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now run the other queries in sequence \n",
    "\n",
    "This enables you to gather your cohort with the first query and then cross-filter the results with subsequent queries. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Queries\n",
    "\n",
    "# Finds key patient demographics\n",
    "\n",
    "Demographics_Query = \"\"\"\n",
    "SELECT {insert columns of interest - probably patient table or like}\n",
    "FROM {main presumably patient table}\n",
    "{LEFT/INNER/OUTER} JOIN {insert tables of interest}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Bleeding Query}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "\"\"\".format(patients_who_bled)\n",
    "\n",
    "# Comorbidity Query\n",
    "\n",
    "Comorbidity_Query = \"\"\"\n",
    "SELECT {insert columns of interest - likely ICD10/SNOMED codes}\n",
    "FROM {main ICD10/SNOMED table}\n",
    "{LEFT/INNER/OUTER} JOIN {other important ICD10/SNOMED tables}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Bleeding Query}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "ORDER BY {perhaps by date or code}\n",
    "\"\"\".format(patients_who_bled)\n",
    "\n",
    "# Admissions Query\n",
    "\n",
    "Admissions_Query = \"\"\"\n",
    "SELECT {insert columns of interest - likely admissions data}\n",
    "FROM {main discharge summary table}\n",
    "{LEFT/INNER/OUTER} JOIN {other important tables if needed}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Bleeding Query}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "\"\"\".format(patients_who_bled)\n",
    "\n",
    "# Physiology Query\n",
    "\n",
    "Physiology_Query = \"\"\"\n",
    "SELECT {insert columns of interest - likely weight and height rather than BMI}\n",
    "FROM {main physiology data table}\n",
    "{LEFT/INNER/OUTER} JOIN {probably not required}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Bleeding Query and perhaps time}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "\"\"\".format(patients_who_bled)\n",
    "\n",
    "# Ward Moves Query\n",
    "\n",
    "Ward_Moves_Query = \"\"\"\n",
    "SELECT {insert columns of interest - ward moves list}\n",
    "FROM {ward moves table}\n",
    "{LEFT/INNER/OUTER} JOIN {could be joined to admissions query but will create greater complexity later on so not done here}\n",
    "WHERE {filters of interest - in this case filtered on the patient number from the Bleeding Query and perhaps time}\n",
    "PATIENT_IDENTIFIER IN {} /* inserts the patient identifiers */\n",
    "\"\"\".format(patients_who_bled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can now connect your engine to these queries to create the dataframes\n",
    "\n",
    "Now by connecting the engines to the queries you can pull all relevant data through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes it very easy to pull discrete up to date datasets from your EPR/PAS into the kernel\n",
    "\n",
    "df_demographics = pd.read_sql_query(Demographics_Query, engine)\n",
    "df_comorbidity = pd.read_sql_query(Comorbidity_Query, engine)\n",
    "df_admissions = pd.read_sql_query(Admissions_Query, engine)\n",
    "df_physiology = pd.read_sql_query(Physiology_Query, engine)\n",
    "df_ward_moves = pd.read_sql_query(Ward_Moves_Query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Index Database\n",
    "\n",
    "In this case we need to build the index dataframe first as we are only interested in index or first admissions. To get these we combine the bleed and admission databases and filter them to extract only the first relevant admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime Function - to turn all column strings containing 'date' or 'datetime' into datetimes\n",
    "\n",
    "def Datetime(series):\n",
    "    if ('date' in series.name.lower() or 'datetime' in series.name.lower()) and 'age' not in series.name.lower():\n",
    "            series = pd.to_datetime(series, dayfirst = True)\n",
    "    return series\n",
    "\n",
    "# First convert all the relevant rows in the key dataframes\n",
    "\n",
    "df_admissions = df_admissions.apply(Datetime)\n",
    "df_bleeding = df_bleeding.apply(Datetime)\n",
    "\n",
    "# Then join the tables so we can select only the relevant admissions\n",
    "\n",
    "df_bleeds_and_admissions = pd.merge(df_bleeding, df_admissions, on='patient_id', how='left')\n",
    "\n",
    "# Now filter to select only the bleed endoscopies which occurred during an admission\n",
    "\n",
    "df_admitted_bleeds = df_bleeds_and_admissions[df_bleeds_and_admissions['date_of_endoscopy'].between(df_bleeds_and_admissions['admission_date'], df_bleeds_and_admissions['discharge_date'])].copy()\n",
    "\n",
    "# Finally select out only the first or index admission as this is what we are interested in. Subsequent admissions are handled seperately\n",
    "\n",
    "df_first_adm = df_admitted_bleeds.loc[df_admitted_bleeds.groupby('date_of_endoscopy')['admission_date'].idxmin()].copy()\n",
    "\n",
    "# Sense Check\n",
    "print(df_first_adm.shape)\n",
    "\n",
    "# Finally calculate length of stay in days as an integer\n",
    "\n",
    "df_first_adm['length_of_stay'] = df_first_adm['discharge_date'] - df_first_adm['admission_date']\n",
    "df_first_adm['length_of_stay'] = df_first_adm['length_of_stay'].dt.days()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to assess only the relevant BMI of these patients\n",
    "\n",
    "We do not want to have all the BMI's - only those that pertain to the time of the gastrointestinal bleeds. We can collect this using a similar strategy to the above one of filtering the values to make sure they occur only after the index admission. First we can assemble the BMI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert the physiology data to contain datetimes\n",
    "\n",
    "df_physiology = df_physiology.apply(Datetime)\n",
    "\n",
    "# Then join the tables so we can select only the relevant physiological measurements\n",
    "\n",
    "df_physiology_admissions = pd.merge(df_first_adm, df_physiology, on='patient_id', how='left')\n",
    "\n",
    "# Now filter df_physiology by the admission dates\n",
    "\n",
    "df_relevant_physiology = df_physiology_admissions[df_physiology_admissions['measurement_date'].between(df_physiology_admissions['admission_date'], df_physiology_admissions['discharge_date'])].copy()\n",
    "\n",
    "# Now extract the weights and heights from the data to create two seperate pandas series\n",
    "\n",
    "df_heights = df_relevant_physiology[df_relevant_physiology['test_code'] == 'HEIG'].copy()\n",
    "df_weights = df_relevant_physiology[df_relevant_physiology['test_code'] == 'WEIG'].copy()\n",
    "\n",
    "# Then aggregate them - we settled for mean in the end after checking for skewness\n",
    "\n",
    "df_heights2 = df_heights.groupby(['patient_id']).agg('mean').reset_index()\n",
    "df_weights2 = df_weights.groupby(['patient_id']).agg('mean').reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "\n",
    "df_weights2.columns = ['PATIENT_ID', 'MEAN_WEIGHT']\n",
    "df_heights2.columns = ['PATIENT_ID', 'MEAN_HEIGHT']\n",
    "\n",
    "# Join Together\n",
    "df_bmi = pd.concat([df_heights2, df_weights2])\n",
    "\n",
    "# Then convert height to meters if currently in centimeters and calc BMI\n",
    "\n",
    "df_bmi['MEAN_HEIGHT_M'] = df_bmi['MEAN_HEIGHT']/100\n",
    "df_bmi['BMI'] = round(df_bmi['MEAN_WEIGHT']/(df_bmi['MEAN_HEIGHT_M'])**2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can usefully merge this with the demographics and the index to create a clean dataframe including the BMI's and age/sex etc\n",
    "\n",
    "This will get us most of the way to having clean / assembled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First merge the core cohort to the demographics and then add the BMI data\n",
    "\n",
    "df_merge1 = pd.merge(df_first_adm, df_demographics, on='patient_id', how = 'left')\n",
    "df_merge2 = pd.merge(df_merge1, df_bmi, on='patient_id', how = 'left')\n",
    "\n",
    "# sense check to make sure nothing has obviously gone wrong\n",
    "\n",
    "df_merge2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can add the comorbidities data as per the learning from death script\n",
    "\n",
    "This part is identical to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a subframe with the key columns\n",
    "\n",
    "comorbidities = df_merge2[['code', 'patient_id', 'age']].copy()\n",
    "comorbidities.columns = ['code', 'id', 'age']\n",
    "\n",
    "# Then calculate the scores. Comorbidipy needs them in this format to work properly\n",
    "\n",
    "cci = comorbidity(comorbidities)\n",
    "frail = hfrs(comorbidities)\n",
    "\n",
    "# Then tidy up the outputs a bit\n",
    "\n",
    "cci['survival_10yr'] = round(cci['survival_10yr']*100,1).astype(str)\n",
    "cci['survival_10yr'] = cci['survival_10yr'].apply(lambda x: ''.join(x + \"%\"))\n",
    "\n",
    "# Then join these tables into one and add back to the main dataframe to create merge3\n",
    "\n",
    "comorb_merge = pd.merge(cci, frail, on='id', how='left')\n",
    "df_merge3 = pd.merge(df_merge2, comorb_merge, left_on='patient_id', right_on='id', how='left')\n",
    "\n",
    "# Sense check to make sure that nothing has obviously gone wrong\n",
    "\n",
    "df_merge3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we need to flag the data such that we can usefully identify our two cohorts\n",
    "\n",
    "The GIBLU cohort all exist within a particular geographic footprint in the hospital. We can identify this cohort and seperate it from the rest by geography and time. We need to know that the GI bleed co-incided with them being admitted to that ward area afterwards. If not they need to be excluded from the cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert the ward moves data to datetimes where relevant\n",
    "\n",
    "df_ward_moves = df_ward_moves.apply(Datetime)\n",
    "\n",
    "# Then join the tables so we can select only the relevant physiological measurements\n",
    "\n",
    "df_moves = pd.merge(df_bleeding, df_ward_moves, on='patient_id', how='left')\n",
    "\n",
    "# Now filter df_moves by the admission dates to find only relevant moves directly after the endoscopy\n",
    "\n",
    "df_relevant_moves = df_moves[df_moves['move_date'] > df_moves['date_of_endoscopy']].copy()\n",
    "df_first_move = df_relevant_moves.loc[df_relevant_moves.groupby('date_of_endoscopy')['move_date'].idxmin()].copy()\n",
    "\n",
    "# Now filter the moves to only those where the patient landed in the target zone of in this case our GIBLU footrpint post procedure\n",
    "# and assign a 1 if the patient landed there - assigning a 0 if they landed elsewhere\n",
    "\n",
    "df_first_move['giblu'] = df_first_move['move_location'].apply(lambda x: 1 if x == 'GIBLU' else 0)\n",
    "\n",
    "# This is great but by now we have more data than we need. What we need to know is whether the scope was followed by move to GIBLU or not\n",
    "# We can therefore discard most of the data prior to rejoining it with the main dataframe\n",
    "# This is the simplest way to cut them. We need to first print the columns\n",
    "\n",
    "print(df_first_move.columns)\n",
    "\n",
    "# Then reassign these columns jettisoning all but the ones we want to keep - as below:\n",
    "\n",
    "df_first_move = df_first_move[['patient_id','date_of_endoscopy','giblu']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining it all up\n",
    "\n",
    "We now have all the parts we need for the analysis apart from rescopes, mortality and re-admission which we will work out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Merge\n",
    "\n",
    "df_merge4 = pd.merge(df_merge3, df_first_move, left_on=['patient_id','date_of_endoscopy'], right_on=['patient_id','date_of_endoscopy'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also now need to work out if the patients were re-scoped or died <30 days\n",
    "\n",
    "We can do this by looking to see if two scopes were performed during the same admission. This can be used to calculate the re-scope rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Re-scopes\n",
    "\n",
    "rescope_counter = df_admitted_bleeds.groupby(['patient_id', 'admission_date'])['date_of_endoscopy'].agg('count')\n",
    "rescope_counter.columns = ['rescopes']\n",
    "rescope_counter['rescoped'] = rescope_counter.rescopes.apply(lambda x: 1 if x> 1 else 0)\n",
    "\n",
    "# This can also be merged in to add this new dimension \n",
    "\n",
    "df_merge5 = pd.merge(df_merge4, df_rescope_counter, left_on=['patient_id','date_of_endoscopy'], right_on=['patient_id','date_of_endoscopy'], how='left')\n",
    "\n",
    "# Now we can calculate the time to death as date of death is in the demographics table already\n",
    "\n",
    "df_merge5['time_to_death'] = df_merge5['date_of_death'] - df_merge5['date_of_endoscopy']\n",
    "\n",
    "# We can use this to work out if 30 days elapsed between the procedure and dying or not (True/False)\n",
    "\n",
    "df_merge5['30_day_mortality'] = df_merge5['time_to_death'] <= timedelta(days=30)\n",
    "\n",
    "# To convert this to an int we can just convert it to an integer type\n",
    "\n",
    "df_merge5['30_day_mortality'] = df_merge5['30_day_mortality'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are really nearly there now. Only the 7 day re-admission rate left to calculate\n",
    "\n",
    "This is a trickier one because it requires slightly more logic than the other outcomes. Patients can only be dead or alive as that event only occurs once but readmission is both a patient-specific event and can either occur or not occur. For this we will use the shift technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now finally we can work out whether the patient was readmitted within 7 days. First we create a simpler frame to work on\n",
    "\n",
    "df_first_adm = df_first_adm[['patient_id', 'date_of_endoscopy', 'admission_date', 'discharge_date']].copy()\n",
    "\n",
    "# Now we need to left join this with the admissions only joining on patient ID\n",
    "\n",
    "df_readmissions = pd.merge(df_first_adm, df_admissions, on='patient_id', how='left')\n",
    "\n",
    "# Now we sort the admissions by date and patient id\n",
    "\n",
    "df_readmissions = df_readmissions.sort_values(['patient_id', 'admission_date'])\n",
    "\n",
    "# Next we take an earlier frame as above and then shift the admission dates up which occurred for the same patient\n",
    "\n",
    "df_readmissions['subsequent_admissions'] = df_readmissions.groupby('patient_id')['admission_date'].shift(-1)\n",
    "\n",
    "# This then allows us to see what the time lag was between procedure date and subsequent admission\n",
    "\n",
    "df_readmissions['time_to_next admission'] = df_readmissions['time_to_next admission'] - df_readmissions['date_of_endoscopy']\n",
    "\n",
    "# We can now calculate if there was a re-admission within 7 days of the endoscopy\n",
    "\n",
    "df_readmissions['readmitted_within_7_days'] = df_readmissions['time_to_next admission'] <= timedelta(days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can join it all up\n",
    "\n",
    "Now we need to join everything together. This has ben quite a verbose way of doing this but I wanted to make sure each step was clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the dataframe down to reduce duplication\n",
    "\n",
    "df_readmissions = df_readmissions[['patient_id', 'admission_date', 'date_of_endoscopy', 'time_to_next admission', 'readmitted_within_7_days']\n",
    "\n",
    "# Final Merge Operation\n",
    "\n",
    "df_merge6 = pd.merge(df_merge5, df_readmissions, left_on=['patient_id','date_of_endoscopy'], right_on=['patient_id','date_of_endoscopy'], how='left')                                \n",
    "                                  \n",
    "# Final Output to tidy things up for the finally prepared data\n",
    "\n",
    "final = df_merge6[['patient_id', 'age', 'sex', 'age_adj_quan_wt_charlson_icd10_quan', 'survival_10yr', 'hfrs', 'giblu', 'rescoped', 'length_of_stay', 'readmitted_within_7_days', '30_day_mortality']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Output\n",
    "\n",
    "We can now output the final result for storage or plumb it directly into an analytics pipeline. It is typically better to split up your preparation or ETL workload from your analytics workload aiming to automate both but in seperate peices. The simplest way to store the output is as a .csv file which works fine in this use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate timedeltas for the output files\n",
    "\n",
    "start_of_data_string = str(df_first_adm['admission_date'].min())\n",
    "end_of_data_string = (str(df_first_adm['discharge_date'].max())\n",
    "\n",
    "# Output the file\n",
    "\n",
    "final.to_csv('{path to output location}' + 'GIBLU Cleaned Data Between {} and {}.csv'.format(start_of_data_string, end_of_data_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you think?\n",
    "\n",
    "We share this as an example of how one can easily use python to take a handful of extracts and process them into some very interesting clean data algorithmically. \n",
    "\n",
    "Some of this script will be tricky for beginners but with practice most people should be able to get their heads around it without too much difficulty. \n",
    "\n",
    "Feel free to contact me at matt@reallyusefulmodels.com if you have any further queries or questions about this, need help or simply want to point out potential improvements / share how you have built upon this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
