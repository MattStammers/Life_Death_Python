{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protecting Life Data Analysis Script\n",
    "\n",
    "- Version 1.2.1 - NHS Pycom Version Built - 22/05/2022\n",
    "- Version 1.1.3 - Current Version Demo delivered to Divisional Management - 14/04/2022\n",
    "- Version 1.1.2 - Abstract Submitted to BSG Conference 2022 - 25/02/2022\n",
    "- Version 1.1.1 - Basic MVP Built - 23/02/2022\n",
    "\n",
    "#### Authors:\n",
    "\n",
    "1. Matt Stammers - Consultant Gastroenterolgist and Data Scientist @ AXIS, UHS\n",
    "2. Michael George - Data Engineering Lead @ AXIS, UHS\n",
    "\n",
    "What this Script Does:\n",
    "- Loads in the data prepared by the data preparation script\n",
    "- Performs mass statistical descripitve analysis on the data after dividing the cohorts into two groups\n",
    "- Performs further statistical analysis on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first stage in an analytics package is to load in the analytics packages - ideally keeping this a slim as possible\n",
    "\n",
    "We have tried to use only a fairly simple selection of packages in this analytics pipeline - this then makes it much easier to build upon later on. Where possible we have coded out statistical functions ourselves to make the code even easier to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Key Packages\n",
    "\n",
    "# Import base packages\n",
    "import math\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# Import data manipulation packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import Statistical Packages. We are using Chi2 and Mann Whitney U Test primary here\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import mannwhitneyu\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Import Plotting Packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas settings adjustment\n",
    "\n",
    "Typically when performing analytics I like to adjust the base pandas settings for maximal customisability. This is up to you but if you want to change the settings you can below in any way you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust settings to see entire frame:\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('mode.chained_assignment', 'warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the funciton block. Here we define our own user-generated functions to help with learning\n",
    "\n",
    "We have written these out bespoke. You can just use the ones contained in other packages but for the sake of learning and clarity we have written out the functions bespoke unless they are contained as defaults within numpy and pandas in which case they are highly reliable. If you spot a mistake please let us know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Functions\n",
    "\n",
    "def IQR(x):\n",
    "    \"\"\"Computes Interquartile Range\"\"\"\n",
    "    lb = stats.scoreatpercentile(x, 25) # Calculates 25th percentile\n",
    "    ub = stats.scoreatpercentile(x, 75) # Calculates 75th percentile\n",
    "    return ub - lb # Subtracts one from the other\n",
    "\n",
    "def IQR_lower(x):\n",
    "    \"\"\"Computes Interquartile Range Lower Quartile\"\"\"\n",
    "    lb = stats.scoreatpercentile(x, 25) # Calculates 25th percentile\n",
    "    return lb\n",
    "\n",
    "def IQR_upper(x):\n",
    "    \"\"\"Computes Interquartile Range Upper Quartile\"\"\"\n",
    "    ub = stats.scoreatpercentile(x, 75) # Calculates 75th percentile\n",
    "    return ub\n",
    "\n",
    "def std_lower(x):\n",
    "    \"\"\"Computes Lower Standard Deviation of the Mean\"\"\"\n",
    "    s = np.std(x) # Calculates Standard Deviation\n",
    "    m = np.mean(x) # Calculates Mean\n",
    "    return  round(m-s,4)\n",
    "\n",
    "def std_upper(x):\n",
    "    \"\"\"Computes Upper Standard Deviation of the Mean\"\"\"\n",
    "    s = np.std(x) # Calculates Standard Deviation\n",
    "    m = np.mean(x) # Calculates Mean\n",
    "    return  round(m+s,4)\n",
    "\n",
    "def CI_lower(x):\n",
    "    \"\"\"Computes 95% Confidence Interval Lower Bound of the Mean\"\"\"\n",
    "    alpha = 0.05                       # significance level = 5%\n",
    "    degfree = len(x) - 1                  # degress of freedom\n",
    "    t = stats.t.ppf(1 - alpha/2, degfree)   # 95% confidence t-score \n",
    "    s = np.std(x)          # sample standard deviation \n",
    "    n = len(x)\n",
    "    m = np.mean(x)\n",
    "    return  round(m - (t * s / np.sqrt(n)),4)\n",
    "\n",
    "def CI_upper(x):\n",
    "    \"\"\"Computes 95% Confidence Interval Upper Bound of the Mean\"\"\"\n",
    "    alpha = 0.05                       # significance level = 5%\n",
    "    degfree = len(x) - 1                  # degress of freedom\n",
    "    t = stats.t.ppf(1 - alpha/2, degfree)   # 95% confidence t-score \n",
    "    s = np.std(x)          # sample standard deviation \n",
    "    n = len(x)\n",
    "    m = np.mean(x)\n",
    "    return  round(m + (t * s / np.sqrt(n)),4)\n",
    "\n",
    "def CI_lower_median(x):\n",
    "    \"\"\"Computes 95% Confidence Interval Lower Bound of the Median\"\"\"\n",
    "    alpha = 0.05                       # significance level = 5%\n",
    "    degfree = len(x) - 1                  # degress of freedom\n",
    "    t = stats.t.ppf(1 - alpha/2, degfree)   # 95% confidence t-score \n",
    "    s = np.std(x)          # sample standard deviation \n",
    "    n = len(x)\n",
    "    m = np.median(x)\n",
    "    return  round(m - (t * s / np.sqrt(n)),4)\n",
    "\n",
    "def CI_upper_median(x):\n",
    "    \"\"\"Computes 95% Confidence Interval Upper Bound of the Median\"\"\"\n",
    "    alpha = 0.05                       # significance level = 5%\n",
    "    degfree = len(x) - 1                  # degress of freedom\n",
    "    t = stats.t.ppf(1 - alpha/2, degfree)   # 95% confidence t-score \n",
    "    s = np.std(x)          # sample standard deviation \n",
    "    n = len(x)\n",
    "    m = np.median(x)\n",
    "    return  round(m + (t * s / np.sqrt(n)),4)\n",
    "    \n",
    "# To calculate estimated cumulative distribution functions if required\n",
    "    \n",
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points: n\n",
    "    n = len(data)\n",
    "    # x-data for the ECDF: x\n",
    "    x = np.sort(data)\n",
    "    # y-data for the ECDF: y\n",
    "    y = np.arange(1, n+1) / n\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Datasets\n",
    "\n",
    "Now we are ready to load in the datasets from the processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in clean data\n",
    "\n",
    "df = pd.read_csv('{path_to_clean_data_from_data_processing/clean_data_file.csv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick sense-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly check the data loaded in\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now in a single line we can compute all the major differences between the GIBLU and non-GIBLU patients\n",
    "\n",
    "Using the .agg function we can very quickly get a good idea about the differences between the two cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full aggregated table of results\n",
    "\n",
    "df_agg = df.groupby('GIBLU').agg(['mean', CI_lower, CI_upper, 'median', CI_lower_median, CI_upper_median, IQR_lower, IQR_upper,'sum','count',std_lower, std_upper])\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Results table\n",
    "\n",
    "This next part is not the most elegant way to tabulate the results and associated statistics but it is the easiest to understand way to do it (ie. no for loops required). I suggest you start by doing it this way (by wrote) and then try to write the same out in a series of loops. Then finally in a single loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Unique Dataframes for each metric\n",
    "\n",
    "mean = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='mean'].T\n",
    "mean_CIL = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='CI_lower'].T\n",
    "mean_CIU = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='CI_upper'].T\n",
    "median = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='median'].T\n",
    "CI_lower_median = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='CI_lower_median'].T\n",
    "CI_upper_median = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='CI_upper_median'].T\n",
    "IQR_lower = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='IQR_lower'].T\n",
    "IQR_upper = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='IQR_upper'].T\n",
    "sums = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='sum'].T\n",
    "counts = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='count'].T\n",
    "std_lower = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='std_lower'].T\n",
    "std_upper = df_agg.iloc[:, df_agg.columns.get_level_values(1)=='std_upper'].T\n",
    "\n",
    "# Re-label the columns to make it human readable\n",
    "\n",
    "mean.columns = ['NIBLU Mean', 'GIBLU Mean']\n",
    "mean_CIL.columns = ['NIBLU Mean 95%CI Lower', 'GIBLU Mean 95%CI Lower']\n",
    "mean_CIU.columns = ['NIBLU Mean 95%CI Upper', 'GIBLU Mean 95%CI Upper']\n",
    "median.columns = ['NIBLU Median', 'GIBLU Median']\n",
    "CI_lower_median.columns = ['NIBLU Median 95%CI Lower', 'GIBLU Median 95%CI Lower']\n",
    "CI_upper_median.columns = ['NIBLU Median 95%CI Upper', 'GIBLU Median 95%CI Upper']\n",
    "IQR_lower.columns = ['NIBLU IQR Lower', 'GIBLU IQR Lower']\n",
    "IQR_upper.columns = ['NIBLU IQR Upper', 'GIBLU IQR Upper']\n",
    "IQR_upper.columns = ['NIBLU IQR Upper', 'GIBLU IQR Upper']\n",
    "sums.columns = ['NIBLU sum', 'GIBLU sum']\n",
    "counts.columns = ['NIBLU count', 'GIBLU count']\n",
    "std_lower.columns = ['NIBLU std_lower', 'GIBLU std_lower']\n",
    "std_upper.columns = ['NIBLU std_upper', 'GIBLU std_upper']\n",
    "\n",
    "# Then reset the index\n",
    "\n",
    "means = mean.reset_index(0).reset_index(drop=True)\n",
    "mean_CILs = mean_CIL.reset_index(0).reset_index(drop=True)\n",
    "mean_CIUs = mean_CIU.reset_index(0).reset_index(drop=True)\n",
    "medians = median.reset_index(0).reset_index(drop=True)\n",
    "median_CILs = CI_lower_median.reset_index(0).reset_index(drop=True)\n",
    "median_CIUs = CI_upper_median.reset_index(0).reset_index(drop=True)\n",
    "IQR_lowers = IQR_lower.reset_index(0).reset_index(drop=True)\n",
    "IQR_uppers = IQR_upper.reset_index(0).reset_index(drop=True)\n",
    "sums = sums.reset_index(0).reset_index(drop=True)\n",
    "counts = counts.reset_index(0).reset_index(drop=True)\n",
    "std_lower = std_lower.reset_index(0).reset_index(drop=True)\n",
    "std_upper = std_upper.reset_index(0).reset_index(drop=True)\n",
    "\n",
    "# Then join them all together into a single dataframe\n",
    "\n",
    "join1 = pd.merge(means, mean_CILs, on='level_0')\n",
    "join2 = pd.merge(join1, mean_CIUs, on='level_0')\n",
    "join3 = pd.merge(join2, medians, on='level_0')\n",
    "join4 = pd.merge(join3, median_CILs, on='level_0')\n",
    "join5 = pd.merge(join4, median_CIUs, on='level_0')\n",
    "join6 = pd.merge(join5, IQR_lowers, on='level_0')\n",
    "join7 = pd.merge(join6, IQR_uppers, on='level_0')\n",
    "join8 = pd.merge(join7, sums, on='level_0')\n",
    "join9 = pd.merge(join8, counts, on='level_0')\n",
    "join10 = pd.merge(join9, std_lower, on='level_0')\n",
    "join11 = pd.merge(join10, std_upper, on='level_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You now have a single table of results\n",
    "\n",
    "This is your master results table. At this point to calculate the statistics it is easiest to understand if we split the tables into two and then calculate the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the two cohorts into GIBLU and not intestinal bleed unit - NIBLU ;D\n",
    "\n",
    "GIBLU = df_cohort[df_cohort['GIBLU'] == 1]\n",
    "NIBLU = df_cohort[df_cohort['GIBLU'] == 0]\n",
    "\n",
    "# First test that the mannwhitney U test is working and you can extract the result successfully\n",
    "\n",
    "a = stats.mannwhitneyu(GIBLU['age'],NIBLU['age'])\n",
    "a_p = a[1]\n",
    "print(a_p)\n",
    "\n",
    "# Then build a Chi2*2 dataframe table to calculate this\n",
    "\n",
    "b1 = GIBLU.loc[:,'sex'].sum()\n",
    "b2 = NIBLU.loc[:,'sex'].sum()\n",
    "b3 = GIBLU.loc[:,'sex'].count()-GIBLU.loc[:,'sex'].sum()\n",
    "b4 = NIBLU.loc[:,'sex'].count()-NIBLU.loc[:,'sex'].sum()\n",
    "contingencyb = pd.DataFrame(np.array([[b1, b2], [b3, b4]]), columns=['GIBLU','NIBLU'])\n",
    "print(contingencyb.head())\n",
    "b = stat,p,dof,expected = chi2_contingency(contingencyb)\n",
    "b_p = b[1]\n",
    "print(b_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once you know this is working you can go through the rest \n",
    "\n",
    "The rest of the stats will be either MannWhitneyU tests for continuous or Chi2 for categorical data. For the sake of simplicity these are all programmed explicitly here but as with the above they can more efficiently be performed in a loop or even better using a function. Again start with this and then work up to the latter. I have labelled them all with letters for each covariate to keep it simple (which works fine for a small dataset like this) and '_p' to denote a P value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then add charlson scores\n",
    "\n",
    "c = stats.mannwhitneyu(GIBLU['age_adj_quan_wt_charlson_icd10_quan'],NIBLU['age_adj_quan_wt_charlson_icd10_quan'])\n",
    "c_p = c[1]\n",
    "print(c_p)\n",
    "\n",
    "# And 10 year survival estimates\n",
    "\n",
    "d = stats.mannwhitneyu(GIBLU['survival_10yr'],NIBLU['survival_10yr'])\n",
    "d_p = d[1]\n",
    "print(d_p)\n",
    "\n",
    "# Then hfrs\n",
    "\n",
    "e = stats.mannwhitneyu(GIBLU['hfrs'],NIBLU['hfrs'])\n",
    "e_p = e[1]\n",
    "print(e_p)\n",
    "\n",
    "# Then rescope rate\n",
    "\n",
    "f = stats.mannwhitneyu(GIBLU['rescoped'],NIBLU['rescoped'])\n",
    "f_p = f[1]\n",
    "print(f_p)\n",
    "\n",
    "# Then length of stay\n",
    "\n",
    "g = stats.mannwhitneyu(GIBLU['length_of_stay'],NIBLU['length_of_stay'])\n",
    "g_p = h[1]\n",
    "print(h_p)\n",
    "\n",
    "# Then readmission within 7 days\n",
    "\n",
    "h = stats.mannwhitneyu(GIBLU['readmitted_within_7_days'],NIBLU['readmitted_within_7_days'])\n",
    "h_p = g[1]\n",
    "print(g_p)\n",
    "\n",
    "# And finally 30 day mortality\n",
    "\n",
    "i1 = GIBLU.loc[:,'30_day_mortality'].sum()\n",
    "i2 = NIBLU.loc[:,'30_day_mortality'].sum()\n",
    "i3 = GIBLU.loc[:,'30_day_mortality'].count()-GIBLU.loc[:,'30_day_mortality'].sum()\n",
    "i4 = NIBLU.loc[:,'30_day_mortality'].count()-NIBLU.loc[:,'30_day_mortality'].sum()\n",
    "contingencyi = pd.DataFrame(np.array([[i1, i2], [i3, i4]]), columns=['GIBLU','NIBLU'])\n",
    "print(contingencyi.head())\n",
    "i = stat,p,dof,expected = chi2_contingency(contingencyi)\n",
    "i_p = i[1]\n",
    "print(i_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can bolt this on to our earlier joined dataframe\n",
    "\n",
    "We can just bolt the p_values on. This is obviously a bit of a clumsy way to do it but it makes it much easier to understand thus why we have written it out so verbosely here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the p values to the joined statistical dataframe\n",
    "\n",
    "join11['p'] = np.array([a_p,b_p,c_p,d_p,e_p,f_p,g_p,h_p,i_p], dtype=object)\n",
    "\n",
    "# Then switch the index to match the correct columns\n",
    "\n",
    "join12 = join11.set_index('level_0')\n",
    "join12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, if we want to we can calculate the differences between variables here\n",
    "\n",
    "This is sometimes required for papers and journals and is now obviously very easy to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the differences by wrote you can do\n",
    "\n",
    "join12['Median Differences'] = join12['NIBLU Median'] - join12['GIBLU Median']\n",
    "join12['Median Lower CI Differences'] = join12['NIBLU Median 95%CI Lower'] - join12['GIBLU Median 95%CI Lower']\n",
    "join12['Median Upper CI Differences'] = join12['NIBLU Median 95%CI Upper'] - join12['GIBLU Median 95%CI Upper']\n",
    "join12['Mean Differences'] = join12['NIBLU Mean'] - join12['GIBLU Mean']\n",
    "join12['Mean Lower CI Differences'] = join12['NIBLU Mean 95%CI Lower'] - join12['GIBLU Mean 95%CI Lower']\n",
    "join12['Mean Upper CI Differences'] = join12['NIBLU Mean 95%CI Upper'] - join12['GIBLU Mean 95%CI Upper']\n",
    "join12['Mean Differences'] = join12['Mean Differences']*100\n",
    "join12['Mean Lower CI Differences'] = join12['Mean Lower CI Differences']*100\n",
    "join12['Mean Upper CI Differences'] = join12['Mean Upper CI Differences']*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally to perform logstic regression and build a model we can use the statsmodels.api \n",
    "\n",
    "I will add this in later as the above is probably enough for newcomers. Once we start adding in statistical models some people will be put off. \n",
    "\n",
    "If you want to have this however, please get in contact. If there is demand I will just append it to the bottom of this script. If not I will probably keep it simple for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
